{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network\n",
    "\n",
    "\"Input times weight add a bias activate\" ~Siraj Raval (for each layer neuron)\n",
    "\n",
    "#### Keys\n",
    "\n",
    "- Used when we use **conditional sequential \"memory\"**\n",
    "\n",
    "- good for to time series as sequencing matters\n",
    "\n",
    "- Activation function can be linear or non-linear \n",
    "\n",
    "- in RNN's there is a 3rd weight matrix which connects the current hidden state to the hidden state of the previous hidden state (unique from feed forward NN's)\n",
    "\n",
    "#### RNN Math\n",
    "\n",
    "$$h^{(t)}=f(h^{(t-1)},x^{(t)}; \\theta)$$\n",
    "\n",
    "Loss Function\n",
    "\n",
    "$$L\\big((x^{(1)},...,x^{(\\tau)}),(y^{(1)},...,y^{(\\tau)})\\big)=\\sum_{t} -\\log\\hat{y}_{y^{(t)}}^{(t)}$$\n",
    "\n",
    "\n",
    "#### Reference: \n",
    "\n",
    "RNN (Simple): https://github.com/llSourcell/recurrent_neural_net_demo/blob/master/rnn.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T00:57:32.397846Z",
     "start_time": "2019-04-19T00:57:32.394674Z"
    }
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T01:13:33.681226Z",
     "start_time": "2019-04-19T01:13:33.677506Z"
    }
   },
   "outputs": [],
   "source": [
    "# compute sigmoid nonlinearity\n",
    "def sigmoid(x):\n",
    "    output = 1/(1+np.exp(-x))\n",
    "    return output\n",
    "\n",
    "# convert output of sigmoid function to its derivative\n",
    "def sigmoid_output_to_derivative(output):\n",
    "    return output*(1-output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T01:13:34.468472Z",
     "start_time": "2019-04-19T01:13:34.463741Z"
    }
   },
   "outputs": [],
   "source": [
    "# training dataset generation\n",
    "int2binary = {}\n",
    "binary_dim = 8\n",
    "\n",
    "largest_number = pow(2,binary_dim)\n",
    "binary = np.unpackbits(\n",
    "    np.array([range(largest_number)],dtype=np.uint8).T,axis=1)\n",
    "for i in range(largest_number):\n",
    "    int2binary[i] = binary[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T01:13:34.768820Z",
     "start_time": "2019-04-19T01:13:34.765748Z"
    }
   },
   "outputs": [],
   "source": [
    "# input variables\n",
    "alpha = 0.1\n",
    "input_dim = 2\n",
    "hidden_dim = 16\n",
    "output_dim = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T01:13:35.022114Z",
     "start_time": "2019-04-19T01:13:35.015892Z"
    }
   },
   "outputs": [],
   "source": [
    "# initialize neural network weights\n",
    "synapse_0 = 2*np.random.random((input_dim,hidden_dim)) - 1\n",
    "synapse_1 = 2*np.random.random((hidden_dim,output_dim)) - 1\n",
    "synapse_h = 2*np.random.random((hidden_dim,hidden_dim)) - 1\n",
    "\n",
    "synapse_0_update = np.zeros_like(synapse_0)\n",
    "synapse_1_update = np.zeros_like(synapse_1)\n",
    "synapse_h_update = np.zeros_like(synapse_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T01:13:35.436253Z",
     "start_time": "2019-04-19T01:13:35.431012Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synapse_0_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T01:13:40.372301Z",
     "start_time": "2019-04-19T01:13:35.937645Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error:[5.36567378]\n",
      "Pred:[0 0 0 0 0 0 0 0]\n",
      "True:[1 0 1 1 1 1 1 0]\n",
      "123 + 67 = 0\n",
      "------------\n",
      "Error:[4.11176]\n",
      "Pred:[1 1 1 1 1 1 1 1]\n",
      "True:[0 1 0 1 0 0 0 0]\n",
      "38 + 42 = 255\n",
      "------------\n",
      "Error:[3.84062208]\n",
      "Pred:[1 1 0 1 0 1 0 1]\n",
      "True:[1 0 0 1 1 1 1 1]\n",
      "37 + 122 = 213\n",
      "------------\n",
      "Error:[3.50093866]\n",
      "Pred:[1 0 1 1 0 0 0 0]\n",
      "True:[1 0 1 1 1 0 1 0]\n",
      "124 + 62 = 176\n",
      "------------\n",
      "Error:[1.71561304]\n",
      "Pred:[0 1 1 0 1 1 0 0]\n",
      "True:[0 1 1 0 1 1 0 0]\n",
      "0 + 108 = 108\n",
      "------------\n",
      "Error:[0.62784196]\n",
      "Pred:[1 0 0 0 1 1 0 0]\n",
      "True:[1 0 0 0 1 1 0 0]\n",
      "76 + 64 = 140\n",
      "------------\n",
      "Error:[0.69191746]\n",
      "Pred:[0 0 1 0 1 1 1 1]\n",
      "True:[0 0 1 0 1 1 1 1]\n",
      "47 + 0 = 47\n",
      "------------\n",
      "Error:[0.3280583]\n",
      "Pred:[0 0 0 0 1 0 1 1]\n",
      "True:[0 0 0 0 1 0 1 1]\n",
      "1 + 10 = 11\n",
      "------------\n",
      "Error:[0.3665797]\n",
      "Pred:[1 1 0 0 0 0 1 0]\n",
      "True:[1 1 0 0 0 0 1 0]\n",
      "74 + 120 = 194\n",
      "------------\n",
      "Error:[0.34021234]\n",
      "Pred:[0 1 0 0 1 0 1 0]\n",
      "True:[0 1 0 0 1 0 1 0]\n",
      "52 + 22 = 74\n",
      "------------\n"
     ]
    }
   ],
   "source": [
    "# training logic\n",
    "for j in range(10000):\n",
    "    \n",
    "    # generate a simple addition problem (a + b = c)\n",
    "    a_int = np.random.randint(largest_number/2) # int version\n",
    "    a = int2binary[a_int] # binary encoding\n",
    "\n",
    "    b_int = np.random.randint(largest_number/2) # int version\n",
    "    b = int2binary[b_int] # binary encoding\n",
    "\n",
    "    # true answer\n",
    "    c_int = a_int + b_int\n",
    "    c = int2binary[c_int]\n",
    "    \n",
    "    # where we'll store our best guess (binary encoded)\n",
    "    d = np.zeros_like(c)\n",
    "\n",
    "    overallError = 0\n",
    "    \n",
    "    layer_2_deltas = list()\n",
    "    layer_1_values = list()\n",
    "    layer_1_values.append(np.zeros(hidden_dim))\n",
    "    \n",
    "    # moving along the positions in the binary encoding\n",
    "    for position in range(binary_dim):\n",
    "        \n",
    "        # generate input and output\n",
    "        X = np.array([[a[binary_dim - position - 1],b[binary_dim - position - 1]]])\n",
    "        y = np.array([[c[binary_dim - position - 1]]]).T\n",
    "\n",
    "        # hidden layer (input ~+ prev_hidden)\n",
    "        layer_1 = sigmoid(np.dot(X,synapse_0) + np.dot(layer_1_values[-1],synapse_h))\n",
    "\n",
    "        # output layer (new binary representation)\n",
    "        layer_2 = sigmoid(np.dot(layer_1,synapse_1))\n",
    "\n",
    "        # did we miss?... if so, by how much?\n",
    "        layer_2_error = y - layer_2\n",
    "        layer_2_deltas.append((layer_2_error)*sigmoid_output_to_derivative(layer_2))\n",
    "        overallError += np.abs(layer_2_error[0])\n",
    "    \n",
    "        # decode estimate so we can print it out\n",
    "        d[binary_dim - position - 1] = np.round(layer_2[0][0])\n",
    "        \n",
    "        # store hidden layer so we can use it in the next timestep\n",
    "        layer_1_values.append(copy.deepcopy(layer_1))\n",
    "    \n",
    "    future_layer_1_delta = np.zeros(hidden_dim)\n",
    "    \n",
    "    for position in range(binary_dim):\n",
    "        \n",
    "        X = np.array([[a[position],b[position]]])\n",
    "        layer_1 = layer_1_values[-position-1]\n",
    "        prev_layer_1 = layer_1_values[-position-2]\n",
    "        \n",
    "        # error at output layer\n",
    "        layer_2_delta = layer_2_deltas[-position-1]\n",
    "        # error at hidden layer\n",
    "        layer_1_delta = (future_layer_1_delta.dot(synapse_h.T) + layer_2_delta.dot(synapse_1.T)) * sigmoid_output_to_derivative(layer_1)\n",
    "\n",
    "        # let's update all our weights so we can try again\n",
    "        synapse_1_update += np.atleast_2d(layer_1).T.dot(layer_2_delta)\n",
    "        synapse_h_update += np.atleast_2d(prev_layer_1).T.dot(layer_1_delta)\n",
    "        synapse_0_update += X.T.dot(layer_1_delta)\n",
    "        \n",
    "        future_layer_1_delta = layer_1_delta\n",
    "    \n",
    "\n",
    "    synapse_0 += synapse_0_update * alpha\n",
    "    synapse_1 += synapse_1_update * alpha\n",
    "    synapse_h += synapse_h_update * alpha    \n",
    "\n",
    "    synapse_0_update *= 0\n",
    "    synapse_1_update *= 0\n",
    "    synapse_h_update *= 0\n",
    "    \n",
    "    # print out progress\n",
    "    if(j % 1000 == 0):\n",
    "        print(\"Error:\" + str(overallError))\n",
    "        print(\"Pred:\" + str(d))\n",
    "        print(\"True:\" + str(c))\n",
    "        out = 0\n",
    "        for index,x in enumerate(reversed(d)):\n",
    "            out += x*pow(2,index)\n",
    "        print(str(a_int) + \" + \" + str(b_int) + \" = \" + str(out))\n",
    "        print(\"------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN (More In-Depth)\n",
    "\n",
    "\n",
    "\n",
    "#### Reference\n",
    "RNN (More in-depth): https://www.youtube.com/watch?v=BwmddtPFWtA \n",
    "Github: https://github.com/llSourcell/recurrent_neural_network/blob/master/RNN.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T15:45:56.429522Z",
     "start_time": "2019-04-19T15:45:56.424914Z"
    }
   },
   "outputs": [],
   "source": [
    "data = open('./data/kafka.txt', 'r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T15:46:46.381775Z",
     "start_time": "2019-04-19T15:46:46.375013Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 137628 chars, 80 unique\n"
     ]
    }
   ],
   "source": [
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('data has %d chars, %d unique' % (data_size, vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T16:08:15.594200Z",
     "start_time": "2019-04-19T16:08:15.589072Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'b': 0, 'm': 1, '/': 2, '4': 3, 'T': 4, '(': 5, 'p': 6, 'Q': 7, 'e': 8, 'K': 9, 'M': 10, '-': 11, 't': 12, 'g': 13, 'u': 14, 'I': 15, 'N': 16, 'z': 17, 'f': 18, 'G': 19, 's': 20, '?': 21, '6': 22, 'J': 23, 'U': 24, 'a': 25, 'l': 26, ';': 27, 'P': 28, 'q': 29, 'B': 30, 'R': 31, 'j': 32, 'S': 33, '0': 34, ')': 35, ' ': 36, ',': 37, '\"': 38, 'ç': 39, '!': 40, 'h': 41, 'W': 42, 'k': 43, '5': 44, ':': 45, 'x': 46, '9': 47, 'i': 48, \"'\": 49, 'c': 50, '$': 51, 'w': 52, '.': 53, 'F': 54, 'V': 55, 'Y': 56, 'A': 57, 'd': 58, '1': 59, 'E': 60, 'C': 61, 'o': 62, '\\n': 63, '7': 64, 'r': 65, '8': 66, 'D': 67, '*': 68, 'y': 69, 'v': 70, 'O': 71, 'n': 72, '@': 73, 'X': 74, '3': 75, 'H': 76, 'L': 77, '2': 78, '%': 79}\n",
      "{0: 'b', 1: 'm', 2: '/', 3: '4', 4: 'T', 5: '(', 6: 'p', 7: 'Q', 8: 'e', 9: 'K', 10: 'M', 11: '-', 12: 't', 13: 'g', 14: 'u', 15: 'I', 16: 'N', 17: 'z', 18: 'f', 19: 'G', 20: 's', 21: '?', 22: '6', 23: 'J', 24: 'U', 25: 'a', 26: 'l', 27: ';', 28: 'P', 29: 'q', 30: 'B', 31: 'R', 32: 'j', 33: 'S', 34: '0', 35: ')', 36: ' ', 37: ',', 38: '\"', 39: 'ç', 40: '!', 41: 'h', 42: 'W', 43: 'k', 44: '5', 45: ':', 46: 'x', 47: '9', 48: 'i', 49: \"'\", 50: 'c', 51: '$', 52: 'w', 53: '.', 54: 'F', 55: 'V', 56: 'Y', 57: 'A', 58: 'd', 59: '1', 60: 'E', 61: 'C', 62: 'o', 63: '\\n', 64: '7', 65: 'r', 66: '8', 67: 'D', 68: '*', 69: 'y', 70: 'v', 71: 'O', 72: 'n', 73: '@', 74: 'X', 75: '3', 76: 'H', 77: 'L', 78: '2', 79: '%'}\n"
     ]
    }
   ],
   "source": [
    "char_to_ix = {ch:i for i, ch in enumerate(chars)}\n",
    "ix_to_char = {i:ch for i, ch in enumerate(chars)}\n",
    "print(char_to_ix)\n",
    "print(ix_to_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T16:09:46.371719Z",
     "start_time": "2019-04-19T16:09:46.366857Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "vector_for_char_a = np.zeros((vocab_size, 1))\n",
    "vector_for_char_a[char_to_ix['a']] = 1\n",
    "print(vector_for_char_a.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T16:20:19.427681Z",
     "start_time": "2019-04-19T16:20:19.424436Z"
    }
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "hidden_size = 100\n",
    "seq_length = 25\n",
    "learning_rate = 1e-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wxh = np.random.randn(hidden_size, vocab_size) * 0.01 # input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size) * 0.01 # input to hidden to recurrent weight matrix\n",
    "Why = np.random.randn(vocab_size, hidden_size) * 0.01 # input to hidden to output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model parameters are adjusted during the trainning.\n",
    "\n",
    "*Wxh* are parameters to connect a vector that contain one input to the hidden layer.<br>\n",
    "*Whh* are parameters to connect the hidden layer to itself. This is the Key of the Rnn:\n",
    "Recursion is done by injecting the previous values from the output of the hidden state, to itself at the next iteration.<br>\n",
    "*Why* are parameters to connect the hidden layer to the output\n",
    "\n",
    "bh contains the hidden bias<br>\n",
    "by contains the output bias<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T16:19:43.978909Z",
     "start_time": "2019-04-19T16:19:43.975910Z"
    }
   },
   "outputs": [],
   "source": [
    "bh = np.zeros((hidden_size, 1))\n",
    "by = np.zeros((vocab_size, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions\n",
    "\n",
    "Let's talk about loss... functions which are at the heart of helping our algorithms learn and perform better each go around. Thus \"minimizing\" loss...\n",
    "\n",
    "During \"training\" the loss function:\n",
    "\n",
    "1. forward pass: calculate the next var given a var from training set\n",
    "2. error: calculates the error of the predicted var given the actual var\n",
    "3. backward pass: calculates the gradient\n",
    "\n",
    "Inputs: input, target and previous hidden state\n",
    "Outputs: loss, gradient for each parameters between layers, previous hidden state\n",
    "\n",
    "#### Forward Pass\n",
    "\n",
    "$$h_{t}=\\phi(W x_{t}+U h_{t-1})$$\n",
    "\n",
    "where $x_{t}$ is the vector that encodes the char at position t and $p_{t}$ is the probability for next char\n",
    "\n",
    "\"Dirty\" pseudo-code:\n",
    "\n",
    "`hs = input*Wxh + last_value_of_hidden_state*Whh + bh`<br>\n",
    "`ys = hs*Why + by`<br>\n",
    "`ps = normalized(ys)`<br>\n",
    "    \n",
    "#### Backward Pass\n",
    "\n",
    "Makes uses of \"Back Propogation\" which calculates the gradient for all parameters at once where gradients are calculated in reverse order of the forward pass. \n",
    "\n",
    "\"Dirty\" pseudo-code:\n",
    "\n",
    "`hs = input*Wxh + last_value_of_hidden_state*Whh + bh`<br>\n",
    "`ys = hs*Why + by`<br>\n",
    "\n",
    "The loss for 1 data point:\n",
    "\n",
    "$$p_{k}=\\frac{e^{f_{k}}}{\\sum_{j}e^{f_{j}}}$$\n",
    "\n",
    "$$L_{i}=-\\log(p_{y_{i}})$$\n",
    "\n",
    "#### Chain Rule (Our Ol' Friend from Calc 1)\n",
    "\n",
    "The chain rule is used for finding the derivative of composite functions (multiple functions)\n",
    "\n",
    "$$f'(x)=(g(h(x))'=g'(h(x))h'(x)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T18:16:21.079113Z",
     "start_time": "2019-04-19T18:16:21.064353Z"
    }
   },
   "outputs": [],
   "source": [
    "def lossFun(inputs, targets, hprev):\n",
    "    # empty dict used to store our input states, hidden states, output states & probabilities\n",
    "    xs, hs, ys, ps = {}, {}, {}, {}\n",
    "    # xs = stores One Hot Encoded input chars \n",
    "    # hs = stores hidden state outputs\n",
    "    # ys = stores targets (unnormalized probabilities)\n",
    "    # ps = stores the converted ys & normlizes prob for chars\n",
    "    \n",
    "    hs[-1] = np.copy(hprev) # separate or disassociate hs from hprev \n",
    "    loss = 0 # initialize loss as 0\n",
    "    \n",
    "    #forward pass\n",
    "    for t in range(len(inputs)):\n",
    "        xs[t] = np.zeros((vocab_size, 1))\n",
    "        xs[t][inputs[t]] = 1 \n",
    "        hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
    "        ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
    "        ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n",
    "        loss += -np.log(ps[t][targets[t], 0]) #softmax (cross-entropy loss)\n",
    "\n",
    "    # backward pass: compute gradients going backwards\n",
    "    dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "    dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "    dhnext = np.zeros_like(hs[0])\n",
    "    for t in reversed(range(len(inputs))):\n",
    "        # output probabilities\n",
    "        dy = np.copy(ps[t]) \n",
    "        # derive our 1st gradient\n",
    "        dy[targets[t]] -= 1 #backprop into y\n",
    "        # compute output gradient - output times hidden states transpose\n",
    "        # when we apply the transpose weight matrix,\n",
    "        # we can think intuitively of this as moving the error backward\n",
    "        # through the network, giving us some sort of measure of the error\n",
    "        # at the output of the 1th layer.\n",
    "        # output gradient\n",
    "        dWhy += np.dot(dy, hs[t].T)\n",
    "        # backpropagate!\n",
    "        dh = np.dot(Why.T, dy) + dhnext # backpop into h\n",
    "        dhraw = (1 - hs[t] * hs[t]) * dh #backprop into tanh nonlinearity\n",
    "        dbh += dhraw # derivative of hidden bias\n",
    "        dWxh += np.dot(dhraw, xs[t].T) # derivative of input to hidden layer weight\n",
    "        dWhh += np.dot(dhraw, hs[t-1].T) # derivative of hidden layer to hidden layer weight\n",
    "        dhnext = np.dot(Whh.T, dhraw)\n",
    "    for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "        np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "    return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T17:28:16.192070Z",
     "start_time": "2019-04-19T17:28:16.187508Z"
    }
   },
   "source": [
    "#### Testing Prediction Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T18:16:22.165612Z",
     "start_time": "2019-04-19T18:16:22.131242Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " nBz4Q.N5AST$hECvj8d\n",
      "T57FAx@0G0\n",
      "l-'ajOIi(s85N-!pF!*EU0Xn2A8\"B?U5MkYbY;FF ':1$pmjQvof\",clL;!@HçGKuL4D@QdKxUwDM@Uou3VXH;(7icUn qWm92;D.dRo8JC6hNh,ItX wkB!wHkAt%9bO;;!8hqOt?8/udBj,(y4,1!NC(oUf@vG;J,\n",
      "B!(u8 \n",
      "----\n"
     ]
    }
   ],
   "source": [
    "# predictionm 1 full foward pass\n",
    "def sample(h, seed_ix, n):\n",
    "    \"\"\"\n",
    "    sample a sequence of integers from the model\n",
    "    h is memory state, seed_ix is seed letter for first time step\n",
    "    n is how many characters to predict\n",
    "    \"\"\"\n",
    "    x = np.zeros((vocab_size, 1)) # create empty vector\n",
    "    x[seed_ix] = 1 # customize it for our seed char\n",
    "    ixes = [] # list to store generated chars\n",
    "    \n",
    "    for t in range(n):\n",
    "        \"\"\"\n",
    "        a hidden state at a given time step is a function\n",
    "        of the input at the same time step modified by a weight matrix\n",
    "        added to the hidden state of the previous time step\n",
    "        multiplied by its own hidden state to hidden state matrix\n",
    "        \"\"\"\n",
    "        h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "        y = np.dot(Why, h) + by # unnormalized output\n",
    "        p = np.exp(y) / np.sum(np.exp(y)) # probabilities for next chars\n",
    "        ix = np.random.choice(range(vocab_size), p=p.ravel()) # pick 1 w/ highest probability\n",
    "        x = np.zeros((vocab_size, 1)) # create a vector\n",
    "        x[ix] = 1 # customize it for the predicted char\n",
    "        ixes.append(ix) # add it to the list\n",
    "        \n",
    "    txt = ''.join(ix_to_char[ix] for ix in ixes)\n",
    "    print('----\\n %s \\n----' % (txt,))\n",
    "\n",
    "hprev = np.zeros((hidden_size, 1)) # reset RNN memory\n",
    "sample(hprev, char_to_ix['a'], 200) # predict the 200 next characters given 'a'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training\n",
    "\n",
    "Process:\n",
    "- feed part of the training text with chunks size of `seq_length`\n",
    "- loss function:\n",
    "    - forward pass to calculate all parameters for the model for a given input/output pairs\n",
    "    - backward pass to calculate all gradients\n",
    "- print a sentence from a random seed using the parameters of the network\n",
    "- update the model using Adaptive Gradient Technique Adagrad\n",
    "\n",
    "#### Feed $L$ function w/ inputs & targets\n",
    "\n",
    "Strategy:\n",
    "- create 2 arrays: 1 from data file, another with targets shifted compared to input one\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T18:16:23.251640Z",
     "start_time": "2019-04-19T18:16:23.246125Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs [71, 72, 8, 36, 1, 62, 65, 72, 48, 72, 13, 37, 36, 52, 41, 8, 72, 36, 19, 65, 8, 13, 62, 65, 36]\n",
      "targets [72, 8, 36, 1, 62, 65, 72, 48, 72, 13, 37, 36, 52, 41, 8, 72, 36, 19, 65, 8, 13, 62, 65, 36, 33]\n"
     ]
    }
   ],
   "source": [
    "p = 0\n",
    "\n",
    "inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "print(\"inputs\", inputs)\n",
    "\n",
    "targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "print(\"targets\", targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T17:53:29.706445Z",
     "start_time": "2019-04-19T17:53:29.702495Z"
    }
   },
   "source": [
    "#### Adagrad to update parameters\n",
    "\n",
    "Adagrad is a type of gradient descent strategy\n",
    "\n",
    "$$\\theta_{t}=\\theta_{t-1}=\\frac{\\alpha}{\\sqrt{1+\\sum_{i}^t g_{i}^2}}g_{t}$$\n",
    "\n",
    "where $\\alpha$ = step_size = learning_rate\n",
    "\n",
    "$g_{i}$ = dparam\n",
    "\n",
    "`param += dparam * learning_rate` or $\\alpha$\n",
    "`mem += dparam * dparam` or $g_{i}^2$\n",
    "`param += -learning_rate * dparam / np.sqrt(mem + 1e-8)`\n",
    "\n",
    "#### Smooth Loss\n",
    "\n",
    "Not sure what role using a filtered version of the loss plays here but according to the tutorial: \"It is a way to average the loss on over the last iterations to better track the progress\"\n",
    "\n",
    "`smooth_loss = smooth_loss * 0.999 + loss * 0.001`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T18:17:55.957807Z",
     "start_time": "2019-04-19T18:16:24.055604Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0, loss 109.550675\n",
      "----\n",
      " Y5,at0rbAW)vC3@gE\"Wç,LU\"c7fUJç VIx?nE\n",
      ";eNe2FwKp-wiCw@a5VmK5XPTHayDoL3\"UDKA!mM@tlob /,a)çpYcs7Up2,2gE,*QUEzNxvg6bDY%i*Ok)Nzw8GVo'*;RLimVA(YWEml!O81-zXp'0p?:EY.L9:yfM7UR5Hk-2I)TN\"!T:obM$$CCjIDk2lOBVaD4( \n",
      "----\n",
      "iter 1000, loss 83.627964\n",
      "----\n",
      "  paw age wtGk he fou bort heimk tuawme \"vf tomy lferte coure , Bed ove thulnvapo ad on erk an ang amreve  jaer kvaind am Is\n",
      "te heaneereeher sate ant ifk chave wt. onit of hem os outd c-fhe inrd ff ome \n",
      "----\n",
      "iter 2000, loss 66.080338\n",
      "----\n",
      " n ree wher the rout wor, litea meneer rerertay whefrevelemom saon, coxin tout atpeom him houc soreeriinme dite wat. shangce beast has has mobrathe foreten lasto them mpether at toudte hem if oxt ik kt \n",
      "----\n",
      "iter 3000, loss 57.284709\n",
      "----\n",
      " ithing sraler stsrytr insaed would wuthurhe she awoubd wis him huy teregnte wi at che llacpulgint bomked wher se go if counlasiked he ronbes mleche soratintsiringe ifpor, att Grecppundwer. He easlais  \n",
      "----\n",
      "iter 4000, loss 53.083971\n",
      "----\n",
      " b\n",
      "xeved tithew thece to wos ant was barked thas te toulderstorm thahd if hermithly or dom ligths aid thaly histoughint an teo cimthe woop that shit mabigun wixtcaaly frowe brele ther fiyciniGy tte ad  \n",
      "----\n",
      "iter 5000, loss 55.741128\n",
      "----\n",
      " y onh ale, herk1emiof hive t:uld 1ow 1omiNo wherk on Mv1 peckor,\"1lo ureealy ellettorscreplyering of when ome pofrend ther.\n",
      "\n",
      "1is werer l?ed to Ploqbead drining olk comatwo turci- proverEgor Cuve:s.1is \n",
      "----\n",
      "iter 6000, loss 56.830775\n",
      "----\n",
      "  verving to wimeed to he seres pexsep fromis mis it beccerallesly the d fed wate ase sstouth ave bearlilly, high loed ingan then woutl if bik so I he Hived tick the woly had had It hed that baiply he  \n",
      "----\n",
      "iter 7000, loss 52.506438\n",
      "----\n",
      " retho baight for lryick have his and to herr coukiig by bleds the if the wad ming in o ngoorghis molutsalledy aghe dime awacilpelt. He ho'rcoy ore meads wa herein tout quipenove pfuwoblle vincter that \n",
      "----\n",
      "iter 8000, loss 49.450018\n",
      "----\n",
      " se hay bistneb.\n",
      "\n",
      "lloataling ale cook aby say thee soo wen a hin. dhe befirgor fut'eve and him hof rovefd har vear in touthing abref intow bofrarlened a dom, of wo tented his evere him tot to derm dott \n",
      "----\n",
      "iter 9000, loss 48.229208\n",
      "----\n",
      " ook whor reth ar home abo that bum caching ony couldy. Then abe apr in then hop afliy at do ad harmed in dey wofed veargsay tomeane puthed, what ce to he cake stould of thear wignwer heved herly hern; \n",
      "----\n",
      "iter 10000, loss 47.747148\n",
      "----\n",
      " ed of the chitiHer.\n",
      "\n",
      "Sad stowy apsto'es Qoreed sak alreald folterout foter tad her optint fof ploy'tl thit ris aplenang the veed. lOgs he wroher in immalleF lus sunt it herk empainte.\n",
      ":Find age fore s \n",
      "----\n",
      "iter 11000, loss 54.810229\n",
      "----\n",
      "  Sent, yow 5ef noteasie \"M\n",
      "catiles ho pearfis door exher now bice bratior 6Ix we sene GrutillithE snentgur atmesed On anifare the dosher of wofemeds. AWiPkndiglpl her\n",
      "termaod gavibe cpeaca puse\n",
      "Gomedl \n",
      "----\n",
      "iter 12000, loss 51.544161\n",
      "----\n",
      " that courgor him hat intorsith loor. Antougs exess in thens wap to luss. He thit thool qut appenbly. \"nelly e as frocn anlasten him chet imtainoustrare his ast was on oud mule psody? Satl this Inrece  \n",
      "----\n",
      "iter 13000, loss 48.358595\n",
      "----\n",
      " eactere had wistit outne u the stor bed with ey chower eroul mather to chis afttmef thick, Gregor effller as on have cuk. \n",
      "amse of laus at whlins trackny bee e ohe meent, Gregonk -ftenfulre exoech dlo \n",
      "----\n",
      "iter 14000, loss 46.598196\n",
      "----\n",
      " ad oply a it everestith. Hes he flitt; that that, ow she Greplateing that thach om hit wapitel, he day for's sail. I lare he to sor thas avlome and drother, whab the cheene, ceive bould coully it fwit \n",
      "----\n",
      "iter 15000, loss 46.010467\n",
      "----\n",
      " loo to be theshiod rour to histor punke the deef risher Ind in the sty ritinss reved watefuns all geave bestply it him that a fan bowady afir to herceror, thould more the qusteree ray homple ork the s \n",
      "----\n",
      "iter 16000, loss 49.055916\n",
      "----\n",
      " utiouct dorcuad beporth\" tHr the fathen are buccivas oie mayie.\n",
      "\n",
      "Thaving iit! 1 omman prentior of infticg inded a rowh to the GrechiCing fard anagr.\n",
      "An tus eseno prepltsee fanter to an's witm dsicay*  \n",
      "----\n",
      "iter 17000, loss 51.047836\n",
      "----\n",
      "  fore all so sef pay uf forked he moth that fo ct hod bublle cemw if opididn lead tholr till ont nomlred his ianciull secen he shay tmy. Gul and the did nare rads work, wis conk lought fore we Broued  \n",
      "----\n",
      "iter 18000, loss 48.209153\n",
      "----\n",
      " n. (am nom if the wondgrite frot ho did the his seang conk and ock ins ewout Gregor protutyo care thes his it hourwofout mact of but of ir heckad stit the try Grearn with., llare. That to the pronas a \n",
      "----\n",
      "iter 19000, loss 46.030130\n",
      "----\n",
      " back wint five ovearat quece she bey e eat for bull most beceavest.\" Is sol soy a croned betlath thig fust. She he hay crowil was som door so croch he the his it we then so wounas and as thit had puis \n",
      "----\n",
      "iter 20000, loss 45.359596\n",
      "----\n",
      "  to was of of to his sho bis Yeack bided whok thla ks to warpryattsoble fore work he 7ad cooke wi the lot! ofter ewainway blfork. But and bus wase that he wloof sireg to the's nebter-ed as himseen was \n",
      "----\n",
      "iter 21000, loss 45.134248\n",
      "----\n",
      " ning b they tus wheugs jut lothed the meotaing the keeat hear fript \"mithwher slee thes wneeckely in that and becoecey for arl, on thearsed, a mowe And has herther his room feren the fould ot simpallo \n",
      "----\n",
      "iter 22000, loss 51.053683\n",
      "----\n",
      " otshen, the anement if douke ovena Projett forting pryis tom a\n",
      "drecilly any of abving tiktion anyaint fulle oth thenud fon he ate ay and the lexear, ibd morarw anly, of precacay!1\n",
      "S\n",
      "fanbio wonpiarit?  \n",
      "----\n",
      "iter 23000, loss 48.634980\n",
      "----\n",
      " aid for wank elas ard. Anly and onk, it to now whit the you willh no- her jut probe. They caly nool gearin, thould youren the Pallt to Iw. teping the oft his sear, had could, and nome vereat the woulm \n",
      "----\n",
      "iter 24000, loss 46.116036\n",
      "----\n",
      " ed quteenely, medeagrel theady, the pood avmedior\n",
      "land the with\n",
      "ap bcaced aved them cearg furts, fore hish fto wertere and that was had sareg- the syon her hars, wappery he hos: ever it shat and nould \n",
      "----\n",
      "iter 25000, loss 44.630498\n",
      "----\n",
      "  at cearad Gregreald asain should forning to he wat sevirbe blove ever wenclick, could was seare a would Gret in has he her and in if the wat eqegor preds. Stpeancoust us dear stipp alr: Who was sote  \n",
      "----\n",
      "iter 26000, loss 44.281834\n",
      "----\n",
      " s, and shainer thot awrion to stever to mouned of thep sist5andr in thaw puseding onchor Wis of on. Romeforr, anowe chanly har thein's not warmed grones - to butully clark behing ho Greed, athes thit  \n",
      "----\n",
      "iter 27000, loss 46.785920\n",
      "----\n",
      " nm\n",
      "thearmentomsenind of the mearisicas I\"\n",
      "yInd moch(ing wo pright for (atrestle the criing.'B\", 1ick rorm air cace door butary ay of self the Mrmactinecion aslopsithen of thald. Wotite prreed of rint  \n",
      "----\n",
      "iter 28000, loss 48.931920\n",
      "----\n",
      "  Hid pepepnod was to a pangemed had ray bosts of lay. midess on hofr plaind with, was baid dod so wids, nlancer lith thaisecallint lechisard his nearding soomecaidectiden thut havealle thof whaid to g \n",
      "----\n",
      "iter 29000, loss 46.453060\n",
      "----\n",
      " proomh at Girreach do, that nave, had spos ofidit aboren leste us heally in hind fall no had orrijed beinh, a drow event dowaling tole his her of coull It he when of one not to had had, oned hearding  \n",
      "----\n",
      "iter 30000, loss 44.535401\n",
      "----\n",
      " enut sefred apmaint fathen thit that be minit, stacl of atte farto the fom a pos the sear basing a from, derbenis hearg timonghing bong nofist und in. Show't sheiringwh, that he was anded and Mn, in t \n",
      "----\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-ee79980334d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# forward seq_length chars through the net & fetch gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWxh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWhh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWhy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdbh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdby\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlossFun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhprev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0msmooth_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmooth_loss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.999\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-45-13539b5964d9>\u001b[0m in \u001b[0;36mlossFun\u001b[0;34m(inputs, targets, hprev)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mdWxh\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdhraw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# derivative of input to hidden layer weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mdWhh\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdhraw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# derivative of hidden layer to hidden layer weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mdhnext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWhh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdhraw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdWxh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWhh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWhy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdbh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdby\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdparam\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# clip to mitigate exploding gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # empty array for Adagrad\n",
    "\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n",
    "\n",
    "while n <= 1000*100:\n",
    "    if p+seq_length+1 > len(data) or n == 0:\n",
    "        hprev = np.zeros((hidden_size, 1)) # reset RNN memory\n",
    "        p = 0 # go from start of data\n",
    "    inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "    targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "    \n",
    "    # forward seq_length chars through the net & fetch gradients\n",
    "    loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "    \n",
    "    # sample from the model now & then\n",
    "    if n % 1000 == 0:\n",
    "        print('iter %d, loss %f' % (n, smooth_loss)) # print progress\n",
    "        sample(hprev, inputs[0], 200)\n",
    "        \n",
    "    # perform parameter update w/ Adagrad\n",
    "    for param, dparam, mem in zip([Wxh, Whh, Why, bh, by],\n",
    "                                 [dWxh, dWhh, dWhy, dbh, dby],\n",
    "                                 [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "        mem += dparam * dparam\n",
    "        param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
    "        \n",
    "    p += seq_length # move data pointer\n",
    "    n += 1 # iteration counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
