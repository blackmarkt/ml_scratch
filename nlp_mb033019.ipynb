{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP (Natural Language Processing)\n",
    "\n",
    "#### TL; DR\n",
    "To develop a deeper intuition with NLP or vectorization of words to do sentimental analysis\n",
    "\n",
    "#### Packages for NLP\n",
    "NLTK\n",
    "\n",
    "### Reference\n",
    "\n",
    "Sentdex [video1](https://pythonprogramming.net/tokenizing-words-sentences-nltk-tutorial/)\n",
    "\n",
    "### Key Terms\n",
    "\n",
    "- **Tokenizing**: grouping of text (2 types of separators: sentences and words)\n",
    "- **Corporas**: body of text with same subject/theme\n",
    "- **Lexicon**: words & their meanings\n",
    "- **Stop Words**: \"fluff\" meaningless words that are typically removed\n",
    "- **Stemming**: typically referred to as the process of removing the end of words that connote a different tense\n",
    "- **Lemmatizing**: gets the root of the words in contrast to stemming\n",
    "- **Tagging**: labeling words as nouns, verbs, adjectives, etc...\n",
    "- **Chunking**: phrases of words that contain a noun surrounded by a verb, adverb that are related\n",
    "\n",
    "*[Regular Expressions](https://pythonprogramming.net/regular-expressions-regex-tutorial-python-3/): own language/symbols \n",
    "\n",
    "- **Chinking**: a chink is a chunk that is removed ofrom a chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T17:37:24.007210Z",
     "start_time": "2019-03-31T17:37:22.584703Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping Sentences & Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T17:37:24.011992Z",
     "start_time": "2019-03-31T17:37:24.009190Z"
    }
   },
   "outputs": [],
   "source": [
    "example_text = \"Hello Mr Smith, how are you doing today? The weather is great, \\\n",
    "                and Python is awesome. The sky is pinkish-blue. \\\n",
    "                You shouldn't eat cardboard.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T17:37:24.030431Z",
     "start_time": "2019-03-31T17:37:24.014458Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello Mr Smith, how are you doing today?', 'The weather is great,                 and Python is awesome.', 'The sky is pinkish-blue.', \"You shouldn't eat cardboard.\"]\n"
     ]
    }
   ],
   "source": [
    "print(sent_tokenize(example_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T17:37:24.037707Z",
     "start_time": "2019-03-31T17:37:24.032851Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'Mr', 'Smith', ',', 'how', 'are', 'you', 'doing', 'today', '?', 'The', 'weather', 'is', 'great', ',', 'and', 'Python', 'is', 'awesome', '.', 'The', 'sky', 'is', 'pinkish-blue', '.', 'You', 'should', \"n't\", 'eat', 'cardboard', '.']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(example_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T17:37:24.081772Z",
     "start_time": "2019-03-31T17:37:24.040245Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "Mr\n",
      "Smith\n",
      ",\n",
      "how\n",
      "are\n",
      "you\n",
      "doing\n",
      "today\n",
      "?\n",
      "The\n",
      "weather\n",
      "is\n",
      "great\n",
      ",\n",
      "and\n",
      "Python\n",
      "is\n",
      "awesome\n",
      ".\n",
      "The\n",
      "sky\n",
      "is\n",
      "pinkish-blue\n",
      ".\n",
      "You\n",
      "should\n",
      "n't\n",
      "eat\n",
      "cardboard\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for i in word_tokenize(example_text):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T17:37:24.090786Z",
     "start_time": "2019-03-31T17:37:24.083900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'it', 'ourselves', 'each', 'have', 'your', 'down', 'mightn', 'over', 'all', 'this', 'up', 'be', 'hadn', 'while', \"she's\", \"hasn't\", 'was', 'needn', 'should', 'why', \"it's\", 'above', 'most', 'so', 'if', \"aren't\", 'own', 'for', 'did', 'in', \"that'll\", \"won't\", 'by', 'here', 'mustn', 'where', 'about', 'just', \"don't\", 'then', 'will', 'into', 'further', 'don', \"you'd\", 'his', 'more', 'too', 'herself', 'after', 'them', 'between', 'you', \"wouldn't\", 'yourself', 've', 'with', \"haven't\", 'a', 'having', 'doesn', 'on', 'that', 'hasn', 'any', 'only', 'she', \"weren't\", 'at', \"you've\", 'through', 'off', 'themselves', 'other', 'very', 'd', 'y', 'yours', 'itself', \"wasn't\", \"needn't\", 'because', 'he', 'now', \"you're\", 'or', 'couldn', 'whom', 'hers', 'does', 'they', 'an', 'haven', 'isn', 'can', 'the', 'those', 'him', 'am', 'few', 'same', 'o', \"doesn't\", 'to', 'such', 'won', 'until', 'as', 'ours', 'of', 'i', 'there', 'do', 'her', 'my', \"should've\", 'its', 'll', 'didn', 'ain', 'under', 'what', \"couldn't\", 'being', \"mustn't\", \"hadn't\", \"shouldn't\", 'against', 'out', 'both', 'not', 'shan', 'which', 'doing', 's', \"mightn't\", \"isn't\", 'before', 'had', 'has', 'wouldn', 'no', 'm', 'and', 'once', 'again', 'yourselves', 'from', 'we', 'than', 'himself', 'their', 'when', \"shan't\", 'below', 'some', 'our', 'me', 'myself', 're', 'been', \"you'll\", 'theirs', 'these', 'during', 'how', 'weren', 'but', 't', \"didn't\", 'wasn', 'are', 'ma', 'shouldn', 'who', 'nor', 'is', 'were', 'aren'}\n"
     ]
    }
   ],
   "source": [
    "example_sent = \"This is a sample sentence, showing off the stop words filtration.\"\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T17:37:24.097426Z",
     "start_time": "2019-03-31T17:37:24.092909Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'sample', 'sentence', ',', 'showing', 'off', 'the', 'stop', 'words', 'filtration', '.']\n",
      "['This', 'sample', 'sentence', ',', 'showing', 'stop', 'words', 'filtration', '.']\n"
     ]
    }
   ],
   "source": [
    "word_tokens = word_tokenize(example_sent)\n",
    "\n",
    "filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "\n",
    "print(word_tokens)\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T17:37:24.104029Z",
     "start_time": "2019-03-31T17:37:24.100713Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "example_words = [\"python\", \"pythoner\", \"pythoning\",\n",
    "                \"pythoned\", \"pythonly\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T17:37:24.113773Z",
     "start_time": "2019-03-31T17:37:24.107211Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['python', 'python', 'python', 'python', 'pythonli']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ps.stem(w) for w in example_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T17:37:24.121982Z",
     "start_time": "2019-03-31T17:37:24.116399Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python\n",
      "python\n",
      "python\n",
      "python\n",
      "pythonli\n"
     ]
    }
   ],
   "source": [
    "for w in example_words:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T17:37:24.129009Z",
     "start_time": "2019-03-31T17:37:24.125360Z"
    }
   },
   "outputs": [],
   "source": [
    "new_text = \"It is important to by very pythonly while you are pythoning with python. \\\n",
    "All pythoners have pythoned poorly at least once.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T17:37:24.140368Z",
     "start_time": "2019-03-31T17:37:24.132101Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It\n",
      "is\n",
      "import\n",
      "to\n",
      "by\n",
      "veri\n",
      "pythonli\n",
      "while\n",
      "you\n",
      "are\n",
      "python\n",
      "with\n",
      "python\n",
      ".\n",
      "all\n",
      "python\n",
      "have\n",
      "python\n",
      "poorli\n",
      "at\n",
      "least\n",
      "onc\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "words = word_tokenize(new_text)\n",
    "\n",
    "for w in words:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T17:37:24.145758Z",
     "start_time": "2019-03-31T17:37:24.142967Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T17:37:24.215067Z",
     "start_time": "2019-03-31T17:37:24.148177Z"
    }
   },
   "outputs": [],
   "source": [
    "train_text = state_union.raw('2005-GWBush.txt')\n",
    "sample_text = state_union.raw(\"2006-GWBush.txt\")\n",
    "\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
    "\n",
    "tokenized = custom_sent_tokenizer.tokenize(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T17:37:24.222464Z",
     "start_time": "2019-03-31T17:37:24.217015Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            \n",
    "            chunkGram = r\"\"\"Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}\"\"\"\n",
    "            chunkParser = nltk.RegexpParser(chunkGram)\n",
    "            chunked = chunkParser.parse(tagged)\n",
    "            print(chunked)\n",
    "            for subtree in chunked.subtress(filter=lambda t: t.label() == 'Chunk'):\n",
    "                print(subtree)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T17:37:24.227110Z",
     "start_time": "2019-03-31T17:37:24.224575Z"
    }
   },
   "outputs": [],
   "source": [
    "#process_content()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chinking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T17:37:24.235624Z",
     "start_time": "2019-03-31T17:37:24.229606Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized[5:]:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "\n",
    "            chunkGram = r\"\"\"Chunk: {<.*>+}\n",
    "                                    }<VB.?|IN|DT|TO>+{\"\"\"\n",
    "\n",
    "            chunkParser = nltk.RegexpParser(chunkGram)\n",
    "            chunked = chunkParser.parse(tagged)\n",
    "            print(chunked)\n",
    "#             chunked.draw()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T17:37:24.244795Z",
     "start_time": "2019-03-31T17:37:24.239029Z"
    }
   },
   "outputs": [],
   "source": [
    "#process_content()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Name Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T17:37:24.255760Z",
     "start_time": "2019-03-31T17:37:24.249361Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized[5:]:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            namedEnt = nltk.ne_chunk(tagged, binary=True)\n",
    "            print(namedEnt)\n",
    "#             namedEnt.draw()\n",
    "    except Exception as e:\n",
    "        print(str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T17:37:24.261163Z",
     "start_time": "2019-03-31T17:37:24.258374Z"
    }
   },
   "outputs": [],
   "source": [
    "#process_content()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T17:37:25.832413Z",
     "start_time": "2019-03-31T17:37:24.262987Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\n",
      "cactus\n",
      "goose\n",
      "rock\n",
      "python\n",
      "good\n",
      "best\n",
      "run\n",
      "run\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(lemmatizer.lemmatize(\"cats\"))\n",
    "print(lemmatizer.lemmatize(\"cacti\"))\n",
    "print(lemmatizer.lemmatize(\"geese\"))\n",
    "print(lemmatizer.lemmatize(\"rocks\"))\n",
    "print(lemmatizer.lemmatize(\"python\"))\n",
    "print(lemmatizer.lemmatize(\"better\", pos=\"a\"))\n",
    "print(lemmatizer.lemmatize(\"best\", pos=\"a\"))\n",
    "print(lemmatizer.lemmatize(\"run\"))\n",
    "print(lemmatizer.lemmatize(\"run\",'v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T17:37:25.837298Z",
     "start_time": "2019-03-31T17:37:25.834057Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/marktblack/anaconda3/lib/python3.7/site-packages/nltk/__init__.py\n"
     ]
    }
   ],
   "source": [
    "print(nltk.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T17:37:26.954177Z",
     "start_time": "2019-03-31T17:37:25.841715Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[The King James Bible]\n",
      "\n",
      "The Old Testament of the King James Bible\n",
      "\n",
      "The First Book of Moses:  Called Genesis\n",
      "\n",
      "\n",
      "1:1 In the beginning God created the heaven and the earth.\n",
      "1:2 And the earth was without form, and void; and darkness was upon\n",
      "the face of the deep.\n",
      "And the Spirit of God moved upon the face of the\n",
      "waters.\n",
      "1:3 And God said, Let there be light: and there was light.\n",
      "1:4 And God saw the light, that it was good: and God divided the light\n",
      "from the darkness.\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, PunktSentenceTokenizer\n",
    "from nltk.corpus import gutenberg\n",
    "\n",
    "# sample text\n",
    "sample = gutenberg.raw(\"bible-kjv.txt\")\n",
    "\n",
    "tok = sent_tokenize(sample)\n",
    "\n",
    "for x in range(5):\n",
    "    print(tok[x])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wordnet/ Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T17:37:26.962027Z",
     "start_time": "2019-03-31T17:37:26.958652Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T17:37:26.971127Z",
     "start_time": "2019-03-31T17:37:26.963723Z"
    }
   },
   "outputs": [],
   "source": [
    "syns = wordnet.synsets(\"program\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T17:37:26.977211Z",
     "start_time": "2019-03-31T17:37:26.973120Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plan.n.01\n"
     ]
    }
   ],
   "source": [
    "print(syns[0].name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T17:37:26.983595Z",
     "start_time": "2019-03-31T17:37:26.979343Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a series of steps to be carried out or goals to be accomplished\n"
     ]
    }
   ],
   "source": [
    "print(syns[0].definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T17:37:26.990129Z",
     "start_time": "2019-03-31T17:37:26.986229Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['they drew up a six-step plan', 'they discussed plans for a new bond issue']\n"
     ]
    }
   ],
   "source": [
    "print(syns[0].examples())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T17:37:27.014243Z",
     "start_time": "2019-03-31T17:37:26.992311Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l: Lemma('good.n.01.good')\n",
      "l: Lemma('good.n.02.good')\n",
      "l: Lemma('good.n.02.goodness')\n",
      "l: Lemma('good.n.03.good')\n",
      "l: Lemma('good.n.03.goodness')\n",
      "l: Lemma('commodity.n.01.commodity')\n",
      "l: Lemma('commodity.n.01.trade_good')\n",
      "l: Lemma('commodity.n.01.good')\n",
      "l: Lemma('good.a.01.good')\n",
      "l: Lemma('full.s.06.full')\n",
      "l: Lemma('full.s.06.good')\n",
      "l: Lemma('good.a.03.good')\n",
      "l: Lemma('estimable.s.02.estimable')\n",
      "l: Lemma('estimable.s.02.good')\n",
      "l: Lemma('estimable.s.02.honorable')\n",
      "l: Lemma('estimable.s.02.respectable')\n",
      "l: Lemma('beneficial.s.01.beneficial')\n",
      "l: Lemma('beneficial.s.01.good')\n",
      "l: Lemma('good.s.06.good')\n",
      "l: Lemma('good.s.07.good')\n",
      "l: Lemma('good.s.07.just')\n",
      "l: Lemma('good.s.07.upright')\n",
      "l: Lemma('adept.s.01.adept')\n",
      "l: Lemma('adept.s.01.expert')\n",
      "l: Lemma('adept.s.01.good')\n",
      "l: Lemma('adept.s.01.practiced')\n",
      "l: Lemma('adept.s.01.proficient')\n",
      "l: Lemma('adept.s.01.skillful')\n",
      "l: Lemma('adept.s.01.skilful')\n",
      "l: Lemma('good.s.09.good')\n",
      "l: Lemma('dear.s.02.dear')\n",
      "l: Lemma('dear.s.02.good')\n",
      "l: Lemma('dear.s.02.near')\n",
      "l: Lemma('dependable.s.04.dependable')\n",
      "l: Lemma('dependable.s.04.good')\n",
      "l: Lemma('dependable.s.04.safe')\n",
      "l: Lemma('dependable.s.04.secure')\n",
      "l: Lemma('good.s.12.good')\n",
      "l: Lemma('good.s.12.right')\n",
      "l: Lemma('good.s.12.ripe')\n",
      "l: Lemma('good.s.13.good')\n",
      "l: Lemma('good.s.13.well')\n",
      "l: Lemma('effective.s.04.effective')\n",
      "l: Lemma('effective.s.04.good')\n",
      "l: Lemma('effective.s.04.in_effect')\n",
      "l: Lemma('effective.s.04.in_force')\n",
      "l: Lemma('good.s.15.good')\n",
      "l: Lemma('good.s.16.good')\n",
      "l: Lemma('good.s.16.serious')\n",
      "l: Lemma('good.s.17.good')\n",
      "l: Lemma('good.s.17.sound')\n",
      "l: Lemma('good.s.18.good')\n",
      "l: Lemma('good.s.18.salutary')\n",
      "l: Lemma('good.s.19.good')\n",
      "l: Lemma('good.s.19.honest')\n",
      "l: Lemma('good.s.20.good')\n",
      "l: Lemma('good.s.20.undecomposed')\n",
      "l: Lemma('good.s.20.unspoiled')\n",
      "l: Lemma('good.s.20.unspoilt')\n",
      "l: Lemma('good.s.21.good')\n",
      "l: Lemma('well.r.01.well')\n",
      "l: Lemma('well.r.01.good')\n",
      "l: Lemma('thoroughly.r.02.thoroughly')\n",
      "l: Lemma('thoroughly.r.02.soundly')\n",
      "l: Lemma('thoroughly.r.02.good')\n"
     ]
    }
   ],
   "source": [
    "synonyms = []\n",
    "antonyms = []\n",
    "\n",
    "for syn in wordnet.synsets(\"good\"):\n",
    "    for l in syn.lemmas():\n",
    "        print(\"l:\", l)\n",
    "        synonyms.append(l.name())\n",
    "        if l.antonyms():\n",
    "            antonyms.append(l.antonyms()[0].name())\n",
    "            \n",
    "# print(set(synonyms))\n",
    "# print('\\n')\n",
    "# print(set(antonyms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T17:37:27.147994Z",
     "start_time": "2019-03-31T17:37:27.018573Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9090909090909091\n"
     ]
    }
   ],
   "source": [
    "w1 = wordnet.synset(\"ship.n.01\")\n",
    "w2 = wordnet.synset(\"boat.n.01\")\n",
    "\n",
    "print(w1.wup_similarity(w2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T17:37:27.157247Z",
     "start_time": "2019-03-31T17:37:27.150222Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6956521739130435\n"
     ]
    }
   ],
   "source": [
    "w1 = wordnet.synset(\"ship.n.01\")\n",
    "w2 = wordnet.synset(\"car.n.01\")\n",
    "\n",
    "print(w1.wup_similarity(w2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T17:37:27.166127Z",
     "start_time": "2019-03-31T17:37:27.158998Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "w1 = wordnet.synset(\"cactus.n.01\")\n",
    "w2 = wordnet.synset(\"cat.n.01\")\n",
    "\n",
    "print(w1.wup_similarity(w2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T17:37:27.170195Z",
     "start_time": "2019-03-31T17:37:27.167957Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from nltk.corpus import movie_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T17:37:31.577155Z",
     "start_time": "2019-03-31T17:37:27.171925Z"
    }
   },
   "outputs": [],
   "source": [
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]\n",
    "\n",
    "random.shuffle(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T17:37:31.581213Z",
     "start_time": "2019-03-31T17:37:31.578655Z"
    }
   },
   "outputs": [],
   "source": [
    "# print(documents[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T17:37:34.997842Z",
     "start_time": "2019-03-31T17:37:31.583968Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(',', 77717), ('the', 76529), ('.', 65876), ('a', 38106), ('and', 35576), ('of', 34123), ('to', 31937), (\"'\", 30585), ('is', 25195), ('in', 21822), ('s', 18513), ('\"', 17612), ('it', 16107), ('that', 15924), ('-', 15595)]\n"
     ]
    }
   ],
   "source": [
    "all_words = []\n",
    "for w in movie_reviews.words():\n",
    "    all_words.append(w.lower())\n",
    "\n",
    "all_words = nltk.FreqDist(all_words)\n",
    "print(all_words.most_common(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T17:37:35.002728Z",
     "start_time": "2019-03-31T17:37:34.999686Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "253\n",
      "35\n"
     ]
    }
   ],
   "source": [
    "print(all_words[\"stupid\"])\n",
    "print(all_words['awesome'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting Words to Features w/ NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T17:37:41.329846Z",
     "start_time": "2019-03-31T17:37:35.004680Z"
    }
   },
   "outputs": [],
   "source": [
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]\n",
    "\n",
    "random.shuffle(documents)\n",
    "\n",
    "all_words = []\n",
    "\n",
    "for w in movie_reviews.words():\n",
    "    all_words.append(w.lower())\n",
    "\n",
    "all_words = nltk.FreqDist(all_words)\n",
    "\n",
    "word_features = list(all_words.keys())[:3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T17:37:41.335146Z",
     "start_time": "2019-03-31T17:37:41.331764Z"
    }
   },
   "outputs": [],
   "source": [
    "def find_features(document):\n",
    "    words = set(document)\n",
    "    features = {}\n",
    "    for w in word_features:\n",
    "        features[w] = (w in words)\n",
    "        \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T17:37:42.200432Z",
     "start_time": "2019-03-31T17:37:41.337350Z"
    }
   },
   "outputs": [],
   "source": [
    "# print((find_features(movie_reviews.words('neg/cv000_29416.txt'))))\n",
    "featuresets = [(find_features(rev), category) for (rev, category) in documents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier\n",
    "\n",
    "#### High Level Overview of the algorithm\n",
    "\n",
    "2 main things:\n",
    "1. Naive \"assumption of independence\"\n",
    "2. Bayes formula:\n",
    "\n",
    "$$P(A|B)=\\frac{P(A)P(B|A)}{P(B)}$$\n",
    "\n",
    "or \n",
    "\n",
    "$$\\text{posterior}=\\frac{\\text{prior occurrence}*\\text{likelihood}}{\\text{evidence}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T17:37:42.205233Z",
     "start_time": "2019-03-31T17:37:42.202256Z"
    }
   },
   "outputs": [],
   "source": [
    "train_end = 1900\n",
    "\n",
    "training_set = featuresets[:train_end]\n",
    "testing_set = featuresets[train_end:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T17:37:49.333511Z",
     "start_time": "2019-03-31T17:37:42.207653Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Performance:\n",
      "\n",
      "78.0\n",
      "Most Informative Features\n",
      "              schumacher = True              neg : pos    =     11.7 : 1.0\n",
      "                  regard = True              pos : neg    =     11.0 : 1.0\n",
      "                   sucks = True              neg : pos    =     10.6 : 1.0\n",
      "                  annual = True              pos : neg    =      9.6 : 1.0\n",
      "                bothered = True              neg : pos    =      8.4 : 1.0\n",
      "                  turkey = True              neg : pos    =      8.2 : 1.0\n",
      "           unimaginative = True              neg : pos    =      7.7 : 1.0\n",
      "                 idiotic = True              neg : pos    =      7.2 : 1.0\n",
      "                    mena = True              neg : pos    =      7.0 : 1.0\n",
      "             silverstone = True              neg : pos    =      7.0 : 1.0\n",
      "                  shoddy = True              neg : pos    =      7.0 : 1.0\n",
      "                  suvari = True              neg : pos    =      7.0 : 1.0\n",
      "                    lame = True              neg : pos    =      6.6 : 1.0\n",
      "                 cunning = True              pos : neg    =      6.3 : 1.0\n",
      "                 singers = True              pos : neg    =      6.3 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(training_set)\n",
    "print(\"Naive Bayes Performance:\\n\")\n",
    "print(nltk.classify.accuracy(classifier, testing_set)*100)\n",
    "classifier.show_most_informative_features(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T17:37:49.502904Z",
     "start_time": "2019-03-31T17:37:49.335537Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# pickle\n",
    "# save_classifier = open(\"./data/naivebayes.pickle\",\"wb\")\n",
    "# pickle.dump(classifier, save_classifier)\n",
    "# save_classifier.close()\n",
    "\n",
    "classifier_f = open(\"./data/naivebayes.pickle\", \"rb\")\n",
    "classifier = pickle.load(classifier_f)\n",
    "classifier_f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T17:37:49.671212Z",
     "start_time": "2019-03-31T17:37:49.504763Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70.0\n",
      "Most Informative Features\n",
      "               insulting = True              neg : pos    =     10.2 : 1.0\n",
      "                    sans = True              neg : pos    =      9.0 : 1.0\n",
      "            refreshingly = True              pos : neg    =      8.3 : 1.0\n",
      "              mediocrity = True              neg : pos    =      7.0 : 1.0\n",
      "             bruckheimer = True              neg : pos    =      6.3 : 1.0\n",
      "                   wires = True              neg : pos    =      6.3 : 1.0\n",
      "               dismissed = True              pos : neg    =      6.3 : 1.0\n",
      "             overwhelmed = True              pos : neg    =      6.3 : 1.0\n",
      "                  wasted = True              neg : pos    =      6.0 : 1.0\n",
      "                flawless = True              pos : neg    =      5.9 : 1.0\n"
     ]
    }
   ],
   "source": [
    "print(nltk.classify.accuracy(classifier, testing_set)*100)\n",
    "classifier.show_most_informative_features(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-Learn Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T17:37:49.682166Z",
     "start_time": "2019-03-31T17:37:49.673080Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB, BernoulliNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MulitNomial NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T17:37:53.495838Z",
     "start_time": "2019-03-31T17:37:49.683709Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNB Classifier accuracy percent: 77.0\n"
     ]
    }
   ],
   "source": [
    "mnb_classifier = SklearnClassifier(MultinomialNB())\n",
    "mnb_classifier.train(training_set)\n",
    "print('MNB Classifier accuracy percent:',\n",
    "     (nltk.classify.accuracy(mnb_classifier, testing_set))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T17:37:56.775874Z",
     "start_time": "2019-03-31T17:37:53.497611Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BNB Classifier accuracy percent: 78.0\n"
     ]
    }
   ],
   "source": [
    "bnb_classifier = SklearnClassifier(BernoulliNB())\n",
    "bnb_classifier.train(training_set)\n",
    "print('BNB Classifier accuracy percent:',\n",
    "     (nltk.classify.accuracy(bnb_classifier, testing_set))*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression, SGDClassifier, SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T17:37:56.780979Z",
     "start_time": "2019-03-31T17:37:56.778069Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T17:38:00.361053Z",
     "start_time": "2019-03-31T17:37:56.783160Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marktblack/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression_classifier accuracy percent: 79.0\n"
     ]
    }
   ],
   "source": [
    "LogisticRegression_classifier = SklearnClassifier(LogisticRegression())\n",
    "LogisticRegression_classifier.train(training_set)\n",
    "print(\"LogisticRegression_classifier accuracy percent:\", \n",
    "      (nltk.classify.accuracy(LogisticRegression_classifier, testing_set))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T17:38:04.014173Z",
     "start_time": "2019-03-31T17:38:00.363022Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDClassifier_classifier accuracy percent: 79.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marktblack/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "SGDC_classifier = SklearnClassifier(SGDClassifier())\n",
    "SGDC_classifier.train(training_set)\n",
    "print(\"SGDClassifier_classifier accuracy percent:\", \n",
    "      (nltk.classify.accuracy(SGDC_classifier, testing_set))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T17:38:15.210484Z",
     "start_time": "2019-03-31T17:38:04.015552Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marktblack/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC_classifier accuracy percent: 78.0\n"
     ]
    }
   ],
   "source": [
    "SVC_classifier = SklearnClassifier(SVC())\n",
    "SVC_classifier.train(training_set)\n",
    "print(\"SVC_classifier accuracy percent:\", \n",
    "      (nltk.classify.accuracy(SVC_classifier, testing_set))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T17:38:18.574635Z",
     "start_time": "2019-03-31T17:38:15.212312Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC_classifier accuracy percent: 78.0\n"
     ]
    }
   ],
   "source": [
    "LinearSVC_classifier = SklearnClassifier(LinearSVC())\n",
    "LinearSVC_classifier.train(training_set)\n",
    "print(\"LinearSVC_classifier accuracy percent:\", \n",
    "      (nltk.classify.accuracy(LinearSVC_classifier, testing_set))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T17:38:28.467243Z",
     "start_time": "2019-03-31T17:38:18.576645Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NuSVC_classifier accuracy percent: 82.0\n"
     ]
    }
   ],
   "source": [
    "NuSVC_classifier = SklearnClassifier(NuSVC())\n",
    "NuSVC_classifier.train(training_set)\n",
    "print(\"NuSVC_classifier accuracy percent:\", \n",
    "      (nltk.classify.accuracy(NuSVC_classifier, testing_set))*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Algo's to Create a Voting System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T17:38:28.479271Z",
     "start_time": "2019-03-31T17:38:28.469777Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.classify import ClassifierI\n",
    "from statistics import mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T17:38:28.486846Z",
     "start_time": "2019-03-31T17:38:28.481078Z"
    }
   },
   "outputs": [],
   "source": [
    "class VoteClassifier(ClassifierI):\n",
    "    def __init__(self, *classifiers):\n",
    "        self._classifiers = classifiers\n",
    "\n",
    "    def classify(self, features):\n",
    "        votes = []\n",
    "        for c in self._classifiers:\n",
    "            v = c.classify(features)\n",
    "            votes.append(v)\n",
    "        return mode(votes)\n",
    "\n",
    "    def confidence(self, features):\n",
    "        votes = []\n",
    "        for c in self._classifiers:\n",
    "            v = c.classify(features)\n",
    "            votes.append(v)\n",
    "\n",
    "        choice_votes = votes.count(mode(votes))\n",
    "        conf = choice_votes / len(votes)\n",
    "        return conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T17:38:28.635106Z",
     "start_time": "2019-03-31T17:38:28.488846Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SGDClassifier_classifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-c88a2cbdabf4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m                                   \u001b[0mNuSVC_classifier\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                                   \u001b[0mLinearSVC_classifier\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                                   \u001b[0mSGDClassifier_classifier\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m                                   \u001b[0mmnb_classifier\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                                   \u001b[0mbnb_classifier\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SGDClassifier_classifier' is not defined"
     ]
    }
   ],
   "source": [
    "voted_classifier = VoteClassifier(classifier,\n",
    "                                  NuSVC_classifier,\n",
    "                                  LinearSVC_classifier,\n",
    "                                  SGD_classifier,\n",
    "                                  mnb_classifier,\n",
    "                                  bnb_classifier,\n",
    "                                  LogisticRegression_classifier)\n",
    "\n",
    "print(\"voted_classifier accuracy percent:\", (nltk.classify.accuracy(voted_classifier, testing_set))*100)\n",
    "\n",
    "print(\"Classification:\", voted_classifier.classify(testing_set[0][0]), \"Confidence %:\",voted_classifier.confidence(testing_set[0][0])*100)\n",
    "print(\"Classification:\", voted_classifier.classify(testing_set[1][0]), \"Confidence %:\",voted_classifier.confidence(testing_set[1][0])*100)\n",
    "print(\"Classification:\", voted_classifier.classify(testing_set[2][0]), \"Confidence %:\",voted_classifier.confidence(testing_set[2][0])*100)\n",
    "print(\"Classification:\", voted_classifier.classify(testing_set[3][0]), \"Confidence %:\",voted_classifier.confidence(testing_set[3][0])*100)\n",
    "print(\"Classification:\", voted_classifier.classify(testing_set[4][0]), \"Confidence %:\",voted_classifier.confidence(testing_set[4][0])*100)\n",
    "print(\"Classification:\", voted_classifier.classify(testing_set[5][0]), \"Confidence %:\",voted_classifier.confidence(testing_set[5][0])*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T17:39:17.126509Z",
     "start_time": "2019-03-31T17:39:17.109458Z"
    }
   },
   "outputs": [],
   "source": [
    "short_pos = open(\"./data/positive.txt\", \"r\", encoding='latin-1').read()\n",
    "short_neg = open(\"./data/negative.txt\", \"r\", encoding='latin-1').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T17:39:19.401937Z",
     "start_time": "2019-03-31T17:39:17.796060Z"
    }
   },
   "outputs": [],
   "source": [
    "documents = []\n",
    "\n",
    "for r in short_pos.split('\\n'):\n",
    "    documents.append((r, \"pos\"))\n",
    "    \n",
    "for r in short_neg.split('\\n'):\n",
    "    documents.append((r, \"neg\"))\n",
    "    \n",
    "all_words = []\n",
    "\n",
    "short_pos_words = word_tokenize(short_pos)\n",
    "short_neg_words = word_tokenize(short_neg)\n",
    "\n",
    "for w in short_pos_words:\n",
    "    all_words.append(w.lower())\n",
    "    \n",
    "for w in short_neg_words:\n",
    "    all_words.append(w.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T17:39:19.560272Z",
     "start_time": "2019-03-31T17:39:19.404415Z"
    }
   },
   "outputs": [],
   "source": [
    "all_words = nltk.FreqDist(all_words)\n",
    "\n",
    "word_features = list(all_words.keys())[:5000]\n",
    "\n",
    "def find_features(document):\n",
    "    words = word_tokenize(document)\n",
    "    features = {}\n",
    "    for w in word_features:\n",
    "        features[w] = (w in words)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T17:39:38.686881Z",
     "start_time": "2019-03-31T17:39:19.562436Z"
    }
   },
   "outputs": [],
   "source": [
    "featuresets = [(find_features(rev), category) for (rev, category) in documents]\n",
    "\n",
    "random.shuffle(featuresets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T17:39:38.790448Z",
     "start_time": "2019-03-31T17:39:38.689636Z"
    }
   },
   "outputs": [],
   "source": [
    "training_set = featuresets[:10000]\n",
    "testing_set = featuresets[10000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T17:38:28.647321Z",
     "start_time": "2019-03-31T17:37:22.755Z"
    }
   },
   "outputs": [],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(training_set)\n",
    "print(\"Original Naive Bayes Algo accuracy percent:\", (nltk.classify.accuracy(classifier, testing_set))*100)\n",
    "classifier.show_most_informative_features(15)\n",
    "\n",
    "MNB_classifier = SklearnClassifier(MultinomialNB())\n",
    "MNB_classifier.train(training_set)\n",
    "print(\"MNB_classifier accuracy percent:\", (nltk.classify.accuracy(MNB_classifier, testing_set))*100)\n",
    "\n",
    "BernoulliNB_classifier = SklearnClassifier(BernoulliNB())\n",
    "BernoulliNB_classifier.train(training_set)\n",
    "print(\"BernoulliNB_classifier accuracy percent:\", (nltk.classify.accuracy(BernoulliNB_classifier, testing_set))*100)\n",
    "\n",
    "LogisticRegression_classifier = SklearnClassifier(LogisticRegression())\n",
    "LogisticRegression_classifier.train(training_set)\n",
    "print(\"LogisticRegression_classifier accuracy percent:\", (nltk.classify.accuracy(LogisticRegression_classifier, testing_set))*100)\n",
    "\n",
    "SGDC_classifier = SklearnClassifier(SGDClassifier())\n",
    "SGDC_classifier.train(training_set)\n",
    "print(\"SGDClassifier_classifier accuracy percent:\", (nltk.classify.accuracy(SGDC_classifier, testing_set))*100)\n",
    "\n",
    "LinearSVC_classifier = SklearnClassifier(LinearSVC())\n",
    "LinearSVC_classifier.train(training_set)\n",
    "print(\"LinearSVC_classifier accuracy percent:\", (nltk.classify.accuracy(LinearSVC_classifier, testing_set))*100)\n",
    "\n",
    "NuSVC_classifier = SklearnClassifier(NuSVC())\n",
    "NuSVC_classifier.train(training_set)\n",
    "print(\"NuSVC_classifier accuracy percent:\", (nltk.classify.accuracy(NuSVC_classifier, testing_set))*100)\n",
    "\n",
    "\n",
    "voted_classifier = VoteClassifier(\n",
    "                                  NuSVC_classifier,\n",
    "                                  LinearSVC_classifier,\n",
    "                                  MNB_classifier,\n",
    "                                  BernoulliNB_classifier,\n",
    "                                  LogisticRegression_classifier)\n",
    "\n",
    "print(\"voted_classifier accuracy percent:\", (nltk.classify.accuracy(voted_classifier, testing_set))*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T17:39:43.028717Z",
     "start_time": "2019-03-31T17:39:38.792363Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10664\n"
     ]
    }
   ],
   "source": [
    "import sentiment_mod as s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T17:39:43.098468Z",
     "start_time": "2019-03-31T17:39:43.030737Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('pos', 1.0)\n"
     ]
    }
   ],
   "source": [
    "# positive review\n",
    "print(s.sentiment(\"This movie was awesome! The acting was great, plot was wonderful, and there were pythons...so yea!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T17:39:43.168616Z",
     "start_time": "2019-03-31T17:39:43.101204Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('neg', 1.0)\n"
     ]
    }
   ],
   "source": [
    "# negative review\n",
    "print(s.sentiment(\"This movie was utter junk. There were absolutely 0 pythons. I don't see what the point was at all. Horrible movie, 0/10\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T17:41:39.614920Z",
     "start_time": "2019-03-31T17:41:39.537058Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('neg', 1.0)\n"
     ]
    }
   ],
   "source": [
    "# positive review\n",
    "print(s.sentiment(\"I've seen better\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
