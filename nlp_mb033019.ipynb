{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP (Natural Language Processing)\n",
    "\n",
    "#### TL; DR\n",
    "To develop a deeper intuition with NLP or vectorization of words to do sentimental analysis\n",
    "\n",
    "#### Packages for NLP\n",
    "NLTK\n",
    "\n",
    "### Reference\n",
    "\n",
    "Sentdex [video1](https://pythonprogramming.net/tokenizing-words-sentences-nltk-tutorial/)\n",
    "\n",
    "### Key Terms\n",
    "\n",
    "- **Tokenizing**: grouping of text (2 types of separators: sentences and words)\n",
    "- **Corporas**: body of text with same subject/theme\n",
    "- **Lexicon**: words & their meanings\n",
    "- **Stop Words**: \"fluff\" meaningless words that are typically removed\n",
    "- **Stemming**: typically referred to as the process of removing the end of words that connote a different tense\n",
    "- **Lemmatizing**: gets the root of the words in contrast to stemming\n",
    "- **Tagging**: labeling words as nouns, verbs, adjectives, etc...\n",
    "- **Chunking**: phrases of words that contain a noun surrounded by a verb, adverb that are related\n",
    "\n",
    "*[Regular Expressions](https://pythonprogramming.net/regular-expressions-regex-tutorial-python-3/): own language/symbols \n",
    "\n",
    "- **Chinking**: a chink is a chunk that is removed ofrom a chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-30T17:31:12.893994Z",
     "start_time": "2019-03-30T17:31:11.612056Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping Sentences & Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-30T17:31:12.903286Z",
     "start_time": "2019-03-30T17:31:12.898709Z"
    }
   },
   "outputs": [],
   "source": [
    "example_text = \"Hello Mr Smith, how are you doing today? The weather is great, \\\n",
    "                and Python is awesome. The sky is pinkish-blue. \\\n",
    "                You shouldn't eat cardboard.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-30T17:31:12.973865Z",
     "start_time": "2019-03-30T17:31:12.908816Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello Mr Smith, how are you doing today?', 'The weather is great,                 and Python is awesome.', 'The sky is pinkish-blue.', \"You shouldn't eat cardboard.\"]\n"
     ]
    }
   ],
   "source": [
    "print(sent_tokenize(example_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-30T17:31:12.983993Z",
     "start_time": "2019-03-30T17:31:12.978383Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'Mr', 'Smith', ',', 'how', 'are', 'you', 'doing', 'today', '?', 'The', 'weather', 'is', 'great', ',', 'and', 'Python', 'is', 'awesome', '.', 'The', 'sky', 'is', 'pinkish-blue', '.', 'You', 'should', \"n't\", 'eat', 'cardboard', '.']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(example_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-30T17:31:12.999586Z",
     "start_time": "2019-03-30T17:31:12.989278Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "Mr\n",
      "Smith\n",
      ",\n",
      "how\n",
      "are\n",
      "you\n",
      "doing\n",
      "today\n",
      "?\n",
      "The\n",
      "weather\n",
      "is\n",
      "great\n",
      ",\n",
      "and\n",
      "Python\n",
      "is\n",
      "awesome\n",
      ".\n",
      "The\n",
      "sky\n",
      "is\n",
      "pinkish-blue\n",
      ".\n",
      "You\n",
      "should\n",
      "n't\n",
      "eat\n",
      "cardboard\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for i in word_tokenize(example_text):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-30T17:31:13.014813Z",
     "start_time": "2019-03-30T17:31:13.004677Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set([u'all', u'just', u\"don't\", u'being', u'over', u'both', u'through', u'yourselves', u'its', u'before', u'o', u'don', u'hadn', u'herself', u'll', u'had', u'should', u'to', u'only', u'won', u'under', u'ours', u'has', u\"should've\", u\"haven't\", u'do', u'them', u'his', u'very', u\"you've\", u'they', u'not', u'during', u'now', u'him', u'nor', u\"wasn't\", u'd', u'did', u'didn', u'this', u'she', u'each', u'further', u\"won't\", u'where', u\"mustn't\", u\"isn't\", u'few', u'because', u\"you'd\", u'doing', u'some', u'hasn', u\"hasn't\", u'are', u'our', u'ourselves', u'out', u'what', u'for', u\"needn't\", u'below', u're', u'does', u\"shouldn't\", u'above', u'between', u'mustn', u't', u'be', u'we', u'who', u\"mightn't\", u\"doesn't\", u'were', u'here', u'shouldn', u'hers', u\"aren't\", u'by', u'on', u'about', u'couldn', u'of', u\"wouldn't\", u'against', u's', u'isn', u'or', u'own', u'into', u'yourself', u'down', u\"hadn't\", u'mightn', u\"couldn't\", u'wasn', u'your', u\"you're\", u'from', u'her', u'their', u'aren', u\"it's\", u'there', u'been', u'whom', u'too', u'wouldn', u'themselves', u'weren', u'was', u'until', u'more', u'himself', u'that', u\"didn't\", u'but', u\"that'll\", u'with', u'than', u'those', u'he', u'me', u'myself', u'ma', u\"weren't\", u'these', u'up', u'will', u'while', u'ain', u'can', u'theirs', u'my', u'and', u've', u'then', u'is', u'am', u'it', u'doesn', u'an', u'as', u'itself', u'at', u'have', u'in', u'any', u'if', u'again', u'no', u'when', u'same', u'how', u'other', u'which', u'you', u\"shan't\", u'shan', u'needn', u'haven', u'after', u'most', u'such', u'why', u'a', u'off', u'i', u'm', u'yours', u\"you'll\", u'so', u'y', u\"she's\", u'the', u'having', u'once'])\n"
     ]
    }
   ],
   "source": [
    "example_sent = \"This is a sample sentence, showing off the stop words filtration.\"\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-30T17:31:13.028227Z",
     "start_time": "2019-03-30T17:31:13.020079Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'sample', 'sentence', ',', 'showing', 'off', 'the', 'stop', 'words', 'filtration', '.']\n",
      "['This', 'sample', 'sentence', ',', 'showing', 'stop', 'words', 'filtration', '.']\n"
     ]
    }
   ],
   "source": [
    "word_tokens = word_tokenize(example_sent)\n",
    "\n",
    "filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "\n",
    "print(word_tokens)\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-30T17:31:13.038619Z",
     "start_time": "2019-03-30T17:31:13.032724Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "example_words = [\"python\", \"pythoner\", \"pythoning\",\n",
    "                \"pythoned\", \"pythonly\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-30T17:31:13.055448Z",
     "start_time": "2019-03-30T17:31:13.042833Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['python', u'python', u'python', u'python', u'pythonli']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ps.stem(w) for w in example_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-30T17:31:13.068426Z",
     "start_time": "2019-03-30T17:31:13.060782Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python\n",
      "python\n",
      "python\n",
      "python\n",
      "pythonli\n"
     ]
    }
   ],
   "source": [
    "for w in example_words:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-30T17:31:13.079376Z",
     "start_time": "2019-03-30T17:31:13.073687Z"
    }
   },
   "outputs": [],
   "source": [
    "new_text = \"It is important to by very pythonly while you are pythoning with python. \\\n",
    "All pythoners have pythoned poorly at least once.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-30T17:31:13.099074Z",
     "start_time": "2019-03-30T17:31:13.084430Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It\n",
      "is\n",
      "import\n",
      "to\n",
      "by\n",
      "veri\n",
      "pythonli\n",
      "while\n",
      "you\n",
      "are\n",
      "python\n",
      "with\n",
      "python\n",
      ".\n",
      "all\n",
      "python\n",
      "have\n",
      "python\n",
      "poorli\n",
      "at\n",
      "least\n",
      "onc\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "words = word_tokenize(new_text)\n",
    "\n",
    "for w in words:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-30T17:31:13.109573Z",
     "start_time": "2019-03-30T17:31:13.103765Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-30T17:31:13.221450Z",
     "start_time": "2019-03-30T17:31:13.115738Z"
    }
   },
   "outputs": [],
   "source": [
    "train_text = state_union.raw('2005-GWBush.txt')\n",
    "sample_text = state_union.raw(\"2006-GWBush.txt\")\n",
    "\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
    "\n",
    "tokenized = custom_sent_tokenizer.tokenize(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-30T17:31:13.236872Z",
     "start_time": "2019-03-30T17:31:13.227073Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            \n",
    "            chunkGram = r\"\"\"Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}\"\"\"\n",
    "            chunkParser = nltk.RegexpParser(chunkGram)\n",
    "            chunked = chunkParser.parse(tagged)\n",
    "            print(chunked)\n",
    "            for subtree in chunked.subtress(filter=lambda t: t.label() == 'Chunk'):\n",
    "                print(subtree)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-30T17:31:13.636794Z",
     "start_time": "2019-03-30T17:31:13.241903Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (Chunk PRESIDENT/NNP GEORGE/NNP W./NNP BUSH/NNP)\n",
      "  'S/POS\n",
      "  (Chunk ADDRESS/NNP)\n",
      "  BEFORE/IN\n",
      "  (Chunk A/NNP JOINT/NNP SESSION/NNP)\n",
      "  OF/IN\n",
      "  (Chunk THE/NNP CONGRESS/NNP ON/NNP THE/NNP STATE/NNP)\n",
      "  OF/IN\n",
      "  (Chunk THE/NNP UNION/NNP January/NNP)\n",
      "  31/CD\n",
      "  ,/,\n",
      "  2006/CD\n",
      "  (Chunk THE/NNP PRESIDENT/NNP)\n",
      "  :/:\n",
      "  (Chunk Thank/NNP)\n",
      "  you/PRP\n",
      "  all/DT\n",
      "  ./.)\n",
      "'Tree' object has no attribute 'subtress'\n"
     ]
    }
   ],
   "source": [
    "process_content()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chinking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-30T17:31:13.649448Z",
     "start_time": "2019-03-30T17:31:13.641479Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized[5:]:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "\n",
    "            chunkGram = r\"\"\"Chunk: {<.*>+}\n",
    "                                    }<VB.?|IN|DT|TO>+{\"\"\"\n",
    "\n",
    "            chunkParser = nltk.RegexpParser(chunkGram)\n",
    "            chunked = chunkParser.parse(tagged)\n",
    "            print(chunked)\n",
    "#             chunked.draw()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T01:49:52.848869Z",
     "start_time": "2019-03-31T01:49:52.845280Z"
    }
   },
   "outputs": [],
   "source": [
    "#process_content()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Name Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-30T17:31:18.773084Z",
     "start_time": "2019-03-30T17:31:18.765556Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized[5:]:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            namedEnt = nltk.ne_chunk(tagged, binary=True)\n",
    "            print(namedEnt)\n",
    "#             namedEnt.draw()\n",
    "    except Exception as e:\n",
    "        print(str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-31T01:49:58.473976Z",
     "start_time": "2019-03-31T01:49:58.469982Z"
    }
   },
   "outputs": [],
   "source": [
    "#process_content()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-30T17:37:57.387495Z",
     "start_time": "2019-03-30T17:37:55.300576Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\n",
      "cactus\n",
      "goose\n",
      "rock\n",
      "python\n",
      "good\n",
      "best\n",
      "run\n",
      "run\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(lemmatizer.lemmatize(\"cats\"))\n",
    "print(lemmatizer.lemmatize(\"cacti\"))\n",
    "print(lemmatizer.lemmatize(\"geese\"))\n",
    "print(lemmatizer.lemmatize(\"rocks\"))\n",
    "print(lemmatizer.lemmatize(\"python\"))\n",
    "print(lemmatizer.lemmatize(\"better\", pos=\"a\"))\n",
    "print(lemmatizer.lemmatize(\"best\", pos=\"a\"))\n",
    "print(lemmatizer.lemmatize(\"run\"))\n",
    "print(lemmatizer.lemmatize(\"run\",'v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-30T17:54:43.696911Z",
     "start_time": "2019-03-30T17:54:43.690121Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/nltk/__init__.pyc\n"
     ]
    }
   ],
   "source": [
    "print(nltk.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-30T17:54:41.816368Z",
     "start_time": "2019-03-30T17:54:40.369753Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[The King James Bible]\n",
      "\n",
      "The Old Testament of the King James Bible\n",
      "\n",
      "The First Book of Moses:  Called Genesis\n",
      "\n",
      "\n",
      "1:1 In the beginning God created the heaven and the earth.\n",
      "1:2 And the earth was without form, and void; and darkness was upon\n",
      "the face of the deep.\n",
      "And the Spirit of God moved upon the face of the\n",
      "waters.\n",
      "1:3 And God said, Let there be light: and there was light.\n",
      "1:4 And God saw the light, that it was good: and God divided the light\n",
      "from the darkness.\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, PunktSentenceTokenizer\n",
    "from nltk.corpus import gutenberg\n",
    "\n",
    "# sample text\n",
    "sample = gutenberg.raw(\"bible-kjv.txt\")\n",
    "\n",
    "tok = sent_tokenize(sample)\n",
    "\n",
    "for x in range(5):\n",
    "    print(tok[x])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wordnet/ Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-30T21:07:23.791823Z",
     "start_time": "2019-03-30T21:07:23.786624Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-30T21:07:42.715741Z",
     "start_time": "2019-03-30T21:07:42.698802Z"
    }
   },
   "outputs": [],
   "source": [
    "syns = wordnet.synsets(\"program\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-30T21:09:42.517646Z",
     "start_time": "2019-03-30T21:09:42.512381Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plan.n.01\n"
     ]
    }
   ],
   "source": [
    "print(syns[0].name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-30T21:09:57.796858Z",
     "start_time": "2019-03-30T21:09:57.791614Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a series of steps to be carried out or goals to be accomplished\n"
     ]
    }
   ],
   "source": [
    "print(syns[0].definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-30T21:11:27.561258Z",
     "start_time": "2019-03-30T21:11:27.555877Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'they drew up a six-step plan', u'they discussed plans for a new bond issue']\n"
     ]
    }
   ],
   "source": [
    "print(syns[0].examples())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-30T21:15:08.217219Z",
     "start_time": "2019-03-30T21:15:08.196462Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('l:', Lemma('good.n.01.good'))\n",
      "('l:', Lemma('good.n.02.good'))\n",
      "('l:', Lemma('good.n.02.goodness'))\n",
      "('l:', Lemma('good.n.03.good'))\n",
      "('l:', Lemma('good.n.03.goodness'))\n",
      "('l:', Lemma('commodity.n.01.commodity'))\n",
      "('l:', Lemma('commodity.n.01.trade_good'))\n",
      "('l:', Lemma('commodity.n.01.good'))\n",
      "('l:', Lemma('good.a.01.good'))\n",
      "('l:', Lemma('full.s.06.full'))\n",
      "('l:', Lemma('full.s.06.good'))\n",
      "('l:', Lemma('good.a.03.good'))\n",
      "('l:', Lemma('estimable.s.02.estimable'))\n",
      "('l:', Lemma('estimable.s.02.good'))\n",
      "('l:', Lemma('estimable.s.02.honorable'))\n",
      "('l:', Lemma('estimable.s.02.respectable'))\n",
      "('l:', Lemma('beneficial.s.01.beneficial'))\n",
      "('l:', Lemma('beneficial.s.01.good'))\n",
      "('l:', Lemma('good.s.06.good'))\n",
      "('l:', Lemma('good.s.07.good'))\n",
      "('l:', Lemma('good.s.07.just'))\n",
      "('l:', Lemma('good.s.07.upright'))\n",
      "('l:', Lemma('adept.s.01.adept'))\n",
      "('l:', Lemma('adept.s.01.expert'))\n",
      "('l:', Lemma('adept.s.01.good'))\n",
      "('l:', Lemma('adept.s.01.practiced'))\n",
      "('l:', Lemma('adept.s.01.proficient'))\n",
      "('l:', Lemma('adept.s.01.skillful'))\n",
      "('l:', Lemma('adept.s.01.skilful'))\n",
      "('l:', Lemma('good.s.09.good'))\n",
      "('l:', Lemma('dear.s.02.dear'))\n",
      "('l:', Lemma('dear.s.02.good'))\n",
      "('l:', Lemma('dear.s.02.near'))\n",
      "('l:', Lemma('dependable.s.04.dependable'))\n",
      "('l:', Lemma('dependable.s.04.good'))\n",
      "('l:', Lemma('dependable.s.04.safe'))\n",
      "('l:', Lemma('dependable.s.04.secure'))\n",
      "('l:', Lemma('good.s.12.good'))\n",
      "('l:', Lemma('good.s.12.right'))\n",
      "('l:', Lemma('good.s.12.ripe'))\n",
      "('l:', Lemma('good.s.13.good'))\n",
      "('l:', Lemma('good.s.13.well'))\n",
      "('l:', Lemma('effective.s.04.effective'))\n",
      "('l:', Lemma('effective.s.04.good'))\n",
      "('l:', Lemma('effective.s.04.in_effect'))\n",
      "('l:', Lemma('effective.s.04.in_force'))\n",
      "('l:', Lemma('good.s.15.good'))\n",
      "('l:', Lemma('good.s.16.good'))\n",
      "('l:', Lemma('good.s.16.serious'))\n",
      "('l:', Lemma('good.s.17.good'))\n",
      "('l:', Lemma('good.s.17.sound'))\n",
      "('l:', Lemma('good.s.18.good'))\n",
      "('l:', Lemma('good.s.18.salutary'))\n",
      "('l:', Lemma('good.s.19.good'))\n",
      "('l:', Lemma('good.s.19.honest'))\n",
      "('l:', Lemma('good.s.20.good'))\n",
      "('l:', Lemma('good.s.20.undecomposed'))\n",
      "('l:', Lemma('good.s.20.unspoiled'))\n",
      "('l:', Lemma('good.s.20.unspoilt'))\n",
      "('l:', Lemma('good.s.21.good'))\n",
      "('l:', Lemma('well.r.01.well'))\n",
      "('l:', Lemma('well.r.01.good'))\n",
      "('l:', Lemma('thoroughly.r.02.thoroughly'))\n",
      "('l:', Lemma('thoroughly.r.02.soundly'))\n",
      "('l:', Lemma('thoroughly.r.02.good'))\n"
     ]
    }
   ],
   "source": [
    "synonyms = []\n",
    "antonyms = []\n",
    "\n",
    "for syn in wordnet.synsets(\"good\"):\n",
    "    for l in syn.lemmas():\n",
    "        print(\"l:\", l)\n",
    "        synonyms.append(l.name())\n",
    "        if l.antonyms():\n",
    "            antonyms.append(l.antonyms()[0].name())\n",
    "            \n",
    "# print(set(synonyms))\n",
    "# print('\\n')\n",
    "# print(set(antonyms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-30T21:17:51.949809Z",
     "start_time": "2019-03-30T21:17:51.819278Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.909090909091\n"
     ]
    }
   ],
   "source": [
    "w1 = wordnet.synset(\"ship.n.01\")\n",
    "w2 = wordnet.synset(\"boat.n.01\")\n",
    "\n",
    "print(w1.wup_similarity(w2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-30T21:18:01.839083Z",
     "start_time": "2019-03-30T21:18:01.823715Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.695652173913\n"
     ]
    }
   ],
   "source": [
    "w1 = wordnet.synset(\"ship.n.01\")\n",
    "w2 = wordnet.synset(\"car.n.01\")\n",
    "\n",
    "print(w1.wup_similarity(w2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-30T21:18:45.555256Z",
     "start_time": "2019-03-30T21:18:45.541295Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "w1 = wordnet.synset(\"cactus.n.01\")\n",
    "w2 = wordnet.synset(\"cat.n.01\")\n",
    "\n",
    "print(w1.wup_similarity(w2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-30T21:22:37.210446Z",
     "start_time": "2019-03-30T21:22:37.204865Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from nltk.corpus import movie_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-30T21:26:57.177934Z",
     "start_time": "2019-03-30T21:26:53.057430Z"
    }
   },
   "outputs": [],
   "source": [
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]\n",
    "\n",
    "random.shuffle(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-30T21:26:58.946428Z",
     "start_time": "2019-03-30T21:26:58.941141Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([u'alchemy', u'is', u'steeped', u'in', u'shades', u'of', u'blue', u'.', u'kieslowski', u\"'\", u's', u'blue', u',', u'that', u'is', u'.', u'with', u'its', u'examination', u'of', u'death', u',', u'isolation', u',', u'character', u'restoration', u',', u'and', u'recovery', u'from', u'loss', u',', u'suzanne', u'myers', u\"'\", u'new', u'independent', u'film', u'echoes', u'the', u'polish', u'director', u\"'\", u's', u'internationally', u'-', u'acclaimed', u'1993', u'release', u'.', u'language', u'aside', u',', u'the', u'principal', u'difference', u'between', u'the', u'films', u'is', u'that', u',', u'while', u'kieslowski', u'took', u'great', u'pains', u'to', u'draw', u'us', u'into', u'the', u'main', u'character', u\"'\", u's', u'world', u',', u'alchemy', u'keeps', u'its', u'viewers', u'at', u'arm', u\"'\", u's', u'length', u'.', u'as', u'a', u'result', u',', u'while', u'we', u\"'\", u're', u'able', u'to', u'appreciate', u'the', u'film', u\"'\", u's', u'intellectual', u'tapestry', u',', u'it', u'is', u'emotionally', u'distant', u'.', u'alchemy', u'is', u'divided', u'into', u'three', u'chapters', u':', u'\"', u'charity', u'\"', u',', u'\"', u'faith', u'\"', u',', u'and', u'\"', u'hope', u'\"', u'.', u'while', u'there', u'are', u'common', u'themes', u'and', u'story', u'elements', u'running', u'through', u'all', u'three', u',', u'the', u'single', u'constant', u'is', u'the', u'main', u'character', u',', u'a', u'sculptor', u'/', u'russian', u'translator', u'named', u'louisa', u'(', u'rya', u'kihlstedt', u')', u'.', u'when', u'the', u'film', u'opens', u',', u'she', u\"'\", u's', u'living', u'with', u'her', u'painter', u'boyfriend', u',', u'whom', u'she', u'believes', u'to', u'be', u'cheating', u'on', u'her', u'.', u'following', u'his', u'sudden', u'death', u'in', u'an', u'automobile', u'accident', u',', u'louisa', u\"'\", u's', u'life', u'is', u'thrown', u'into', u'turmoil', u'.', u'events', u'swirls', u'around', u'her', u',', u'and', u'she', u'can', u\"'\", u't', u'find', u'a', u'source', u'of', u'stability', u'.', u'in', u'her', u'quest', u'to', u'make', u'sense', u'out', u'of', u'her', u'loss', u',', u'she', u'befriends', u'her', u'late', u'boyfriend', u\"'\", u's', u'mistress', u',', u'visits', u'her', u'ill', u'sister', u',', u'and', u',', u'eventually', u',', u'abandons', u'her', u'old', u'life', u'by', u'joining', u'a', u'\"', u'back', u'-', u'to', u'-', u'nature', u'\"', u'cult', u'.', u'alchemy', u'successfully', u'explores', u'a', u'variety', u'of', u'compelling', u'issues', u'.', u'in', u'addition', u'to', u'asking', u'the', u'basic', u'question', u'of', u'how', u'a', u'person', u'should', u'cope', u'with', u'the', u'unexpected', u'death', u'of', u'a', u'loved', u'one', u',', u'it', u'probes', u'beneath', u'the', u'surface', u'of', u'concepts', u'like', u'the', u'importance', u'of', u'faith', u'in', u'the', u'healing', u'process', u',', u'the', u'meaning', u'of', u'love', u',', u'and', u'the', u'nature', u'of', u'art', u'.', u'does', u'personality', u'restoration', u'come', u'through', u'interacting', u'with', u'others', u'or', u'escaping', u'from', u'the', u'familiar', u'?', u'are', u'artistic', u'epiphanies', u'the', u'result', u'of', u'focused', u'solitude', u'or', u'of', u'living', u'life', u',', u'with', u'all', u'of', u'its', u'various', u'distractions', u',', u'to', u'the', u'fullest', u'?', u'myers', u\"'\", u'challenge', u'is', u'to', u'make', u'these', u'subjects', u',', u'art', u'film', u'staples', u',', u'engrossing', u'in', u'a', u'new', u'context', u'.', u'through', u'louisa', u\"'\", u's', u'struggles', u',', u'she', u'succeeds', u'.', u'while', u'alchemy', u'isn', u\"'\", u't', u'always', u'emotionally', u'appealing', u',', u'it', u'never', u'loses', u'its', u'fascination', u'.', u'even', u'when', u'we', u\"'\", u're', u'not', u'connecting', u'with', u'louisa', u',', u'who', u'is', u'often', u'more', u'of', u'a', u'locus', u'for', u'ideas', u'than', u'a', u'fully', u'-', u'developed', u'personality', u',', u'there', u\"'\", u's', u'enough', u'material', u'on', u'-', u'screen', u'to', u'hold', u'our', u'attention', u'.', u'the', u'cinematography', u'is', u'stunning', u'.', u'city', u'scenes', u'are', u'tinted', u'with', u'blue', u',', u'adding', u'a', u'cold', u',', u'bleak', u'dimension', u'to', u'the', u'film', u\"'\", u's', u'early', u'portions', u'.', u'later', u',', u'when', u'louisa', u'reaches', u'the', u'\"', u'wilderness', u'\"', u',', u'we', u\"'\", u're', u'treated', u'to', u'a', u'dazzling', u'array', u'of', u'autumnal', u'splendor', u'--', u'brightly', u'-', u'colored', u'leaves', u'in', u'the', u'trees', u'and', u'on', u'the', u'ground', u'.', u'it', u\"'\", u's', u'a', u'startling', u'-', u'but', u'-', u'effective', u'visual', u'contrast', u'that', u'emphasizes', u'the', u'changes', u'taking', u'place', u'in', u'louisa', u\"'\", u's', u'life', u'as', u'she', u'travels', u'the', u'road', u'back', u'to', u'emotional', u'stability', u'.', u'the', u'title', u'refers', u'to', u'the', u'practice', u'of', u'a', u'group', u'of', u'mystical', u'faith', u'-', u'healers', u'who', u'use', u'\"', u'alchemy', u'\"', u'to', u'provide', u'for', u'the', u'needs', u'of', u'the', u'body', u'and', u'soul', u'.', u'and', u',', u'although', u'louisa', u'partakes', u'of', u'their', u'peculiar', u'brand', u'of', u'medicine', u',', u'it', u\"'\", u's', u'ultimately', u'love', u'and', u'companionship', u',', u'not', u'alchemy', u',', u'that', u'brings', u'solace', u'.', u'her', u'pain', u'is', u'assuaged', u'only', u'when', u'she', u'lets', u'go', u'of', u'the', u'belief', u'that', u'the', u'inherent', u'safety', u'of', u'being', u'alone', u'is', u'life', u\"'\", u's', u'ultimate', u'goal', u'.', u'even', u'though', u'louisa', u\"'\", u's', u'final', u'transformation', u'lacks', u'the', u'impact', u'it', u'could', u'have', u'possessed', u'had', u'she', u'been', u'a', u'more', u'vital', u'character', u',', u'it', u\"'\", u's', u'still', u'an', u'intriguing', u'and', u'enlightening', u'process', u'to', u'watch', u'.', u'alchemy', u'isn', u\"'\", u't', u'pure', u'magic', u',', u'but', u',', u'especially', u'during', u'its', u'best', u'moments', u',', u'it', u\"'\", u's', u'close', u'.'], u'pos')\n"
     ]
    }
   ],
   "source": [
    "print(documents[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-30T21:36:30.570169Z",
     "start_time": "2019-03-30T21:36:26.200628Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u',', 77717), (u'the', 76529), (u'.', 65876), (u'a', 38106), (u'and', 35576), (u'of', 34123), (u'to', 31937), (u\"'\", 30585), (u'is', 25195), (u'in', 21822), (u's', 18513), (u'\"', 17612), (u'it', 16107), (u'that', 15924), (u'-', 15595)]\n",
      "253\n",
      "35\n"
     ]
    }
   ],
   "source": [
    "all_words = []\n",
    "for w in movie_reviews.words():\n",
    "    all_words.append(w.lower())\n",
    "\n",
    "all_words = nltk.FreqDist(all_words)\n",
    "print(all_words.most_common(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-30T21:36:34.779855Z",
     "start_time": "2019-03-30T21:36:34.774590Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "253\n",
      "35\n"
     ]
    }
   ],
   "source": [
    "print(all_words[\"stupid\"])\n",
    "print(all_words['awesome'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting Words to Features w/ NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
